{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-24T10:34:22.734439Z","iopub.execute_input":"2025-11-24T10:34:22.734665Z","iopub.status.idle":"2025-11-24T10:34:24.515192Z","shell.execute_reply.started":"2025-11-24T10:34:22.734642Z","shell.execute_reply":"2025-11-24T10:34:24.514370Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# CIFAR\n\n**Project Overview**\n\nGoal:\n\nBuild a Custom CNN (no transfer learning) that can classify CIFAR-100 images into 100 classes with ~65% accuracy.\n\nDataset:\n\n\t•\tCIFAR-100: 50k train + 10k test, each image is 32×32 RGB, 100 classes.\n\t•\tExample classes: apple, chair, shark, pickup truck, leopard, etc.\n\nApproach:\n\t\n    1.\tLoad dataset & prepare data pipeline\n\t2.\tCreate augmentations\n\t3.\tBuild CNN with residual blocks, BatchNorm, Dropout\n\t4.\tTrain with smart learning rate schedule\n\t5.\tEvaluate performance\n**","metadata":{}},{"cell_type":"code","source":"# imports\n\nimport tensorflow as tf\nimport numpy as np\nimport math","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T10:36:08.652614Z","iopub.execute_input":"2025-11-24T10:36:08.653327Z","iopub.status.idle":"2025-11-24T10:36:08.657047Z","shell.execute_reply.started":"2025-11-24T10:36:08.653291Z","shell.execute_reply":"2025-11-24T10:36:08.656289Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from tensorflow.keras import layers, models, regularizers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T10:36:08.658195Z","iopub.execute_input":"2025-11-24T10:36:08.658523Z","iopub.status.idle":"2025-11-24T10:36:08.675107Z","shell.execute_reply.started":"2025-11-24T10:36:08.658498Z","shell.execute_reply":"2025-11-24T10:36:08.674364Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"What’s happening:\n\n\t•\ttensorflow → our deep learning framework\n\t•\tlayers, models, regularizers → Keras tools for building CNNs\n    \n\t•\tnumpy → for array math\n\t•\tmath → for cosine learning rate scheduling","metadata":{}},{"cell_type":"markdown","source":"# PART 3 — Hyperparameter","metadata":{}},{"cell_type":"markdown","source":"Concept:\n\nThink of these like dials on a control board:\n\n\t•\tBATCH_SIZE: number of images per step\n\t•\tEPOCHS: how many passes over the training data\n\t•\tMIXUP_ALPHA: controls MixUp augmentation (image blending)\n\t•\tLABEL_SMOOTHING: prevents overconfidence in predictions\n\t•\tWEIGHT_DECAY: small penalty on weights to avoid                   overfitting\n    \n\t•\tLR (Learning Rate): how fast the model learns\n\t•\tMOMENTUM: helps SGD optimizer “remember” direction\n\t•\tWARMUP: slowly increase LR early to stabilize training","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 128\nIMG_SHAPE = (32, 32, 3)\nNUM_CLASSES = 100\nEPOCHS = 200\n\nMIXUP_ALPHA = 0.2\nLABEL_SMOOTHING = 0.1\nWEIGHT_DECAY = 1e-4\n\nINITIAL_LR = 0.1\nMOMENTUM = 0.9\nWARMUP_EPOCHS = 5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T10:36:08.675717Z","iopub.execute_input":"2025-11-24T10:36:08.675975Z","iopub.status.idle":"2025-11-24T10:36:08.691557Z","shell.execute_reply.started":"2025-11-24T10:36:08.675959Z","shell.execute_reply":"2025-11-24T10:36:08.691045Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# PART 4 — Data Loading\n\nConcept:\n\n\t•\tTensorFlow gives you CIFAR-100 ready to go.\n\t•\tEach x = images, y = class labels.\n\t•\tlabel_mode='fine' → 100 detailed classes (not grouped into 20 superclasses).\n\t•\ttf.squeeze removes unnecessary shape dimensions","metadata":{}},{"cell_type":"code","source":"(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar100.load_data(label_mode = 'fine')\ny_train = tf.squeeze(y_train)\ny_test = tf.squeeze(y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T10:36:08.692309Z","iopub.execute_input":"2025-11-24T10:36:08.692533Z","iopub.status.idle":"2025-11-24T10:36:11.136448Z","shell.execute_reply.started":"2025-11-24T10:36:08.692515Z","shell.execute_reply":"2025-11-24T10:36:11.135822Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"# PART 5 — Data Preprocessing (Train vs Test)\n\nConcept:\n\n\t•\tNormalization: divide by 255 to get pixels between [0,1]\n\t•\tAugmentation (train only): random crop, flip, brightness — makes model more robust\n\t•\tNo augmentation on test data, only normalize","metadata":{}},{"cell_type":"code","source":" # Training Prep\ndef preprocess_train(image, label):\n    image = tf.cast(image, tf.float32) / 255.0\n    image = tf.image.resize_with_crop_or_pad(image, 36, 36)\n    image = tf.image.random_crop(image, [32, 32, 3])\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_brightness(image, 0.1)\n    image = tf.image.random_contrast(image, 0.9, 1.1)\n    return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T10:36:11.138238Z","iopub.execute_input":"2025-11-24T10:36:11.138491Z","iopub.status.idle":"2025-11-24T10:36:11.142519Z","shell.execute_reply.started":"2025-11-24T10:36:11.138475Z","shell.execute_reply":"2025-11-24T10:36:11.141832Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Eval Prep\ndef preprocess_eval(image, label):\n    image = tf.cast(image, tf.float32) / 255.0\n    return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T10:36:11.143347Z","iopub.execute_input":"2025-11-24T10:36:11.143757Z","iopub.status.idle":"2025-11-24T10:36:11.820423Z","shell.execute_reply.started":"2025-11-24T10:36:11.143735Z","shell.execute_reply":"2025-11-24T10:36:11.819597Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"# PART 6 — Build the TensorFlow Dataset Pipeline\n\nConcept:\n\n\t•\ttf.data.Dataset efficiently feeds data to GPU.\n\t•\tshuffle prevents the model from memorizing the order.\n\t•\tmap applies your preprocessing function.\n\t•\tprefetch overlaps data loading with GPU work for speed","metadata":{}},{"cell_type":"code","source":"train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\ntrain_ds = train_ds.shuffle(50000)\ntrain_ds = train_ds.map(preprocess_train, num_parallel_calls = tf.data.AUTOTUNE)\ntrain_ds = train_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\ntest_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\ntest_ds = test_ds.map(preprocess_eval).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T10:36:11.821220Z","iopub.execute_input":"2025-11-24T10:36:11.821492Z","iopub.status.idle":"2025-11-24T10:36:12.773051Z","shell.execute_reply.started":"2025-11-24T10:36:11.821470Z","shell.execute_reply":"2025-11-24T10:36:12.772473Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"# PART 7 — CNN Building Blocks\n\nConcept:\n\n\t•\tConv2D → extracts features (edges, textures, etc.)\n\t•\tBatchNorm → stabilizes and speeds up learning\n\t•\tReLU → adds non-linearity","metadata":{}},{"cell_type":"code","source":"def conv_bn_relu(x, filters, kernel_size=3, stride=1):\n    x = layers.Conv2D(filters, kernel_size, strides=stride, padding='same', use_bias=False)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n    return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T10:36:12.773724Z","iopub.execute_input":"2025-11-24T10:36:12.773961Z","iopub.status.idle":"2025-11-24T10:36:12.778162Z","shell.execute_reply.started":"2025-11-24T10:36:12.773934Z","shell.execute_reply":"2025-11-24T10:36:12.777474Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"# PART 8 — Residual Block\n\nConcept:\n\nResidual blocks = ResNet magic\n\nThey skip connections, letting gradients flow easily through deep networks.\n\nYou can think: “learn the change (residual) instead of starting from scratch each layer\"","metadata":{}},{"cell_type":"code","source":"def residual_block(x, filters, stride=1):\n    shortcut = x\n    out = conv_bn_relu(x, filters, 3, stride)\n    out = layers.Conv2D(filters, 3, padding='same', use_bias=False)(out)\n    out = layers.BatchNormalization()(out)\n\n    # adjust shortcut size if needed\n    if stride != 1 or x.shape[-1] != filters:\n        shortcut = layers.Conv2D(filters, 1, strides=stride, use_bias=False)(shortcut)\n        shortcut = layers.BatchNormalization()(shortcut)\n\n    x = layers.add([out, shortcut])\n    x = layers.ReLU()(x)\n    return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T10:36:12.778828Z","iopub.execute_input":"2025-11-24T10:36:12.779155Z","iopub.status.idle":"2025-11-24T10:36:12.799346Z","shell.execute_reply.started":"2025-11-24T10:36:12.779134Z","shell.execute_reply":"2025-11-24T10:36:12.798819Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"# PART 10 — Full CNN Architecture\n\nConcept:\n\n\t•\tDeep stacked residual layers → feature extraction\n\t•\tGlobalAveragePooling + GlobalMaxPooling → captures both mean and extreme features\n\t•\tDense layer → final classifier\n\t•\tSoftmax → converts to probabilities","metadata":{}},{"cell_type":"code","source":"def build_custom_cnn(input_shape=(32,32,3), num_classes=100):\n    inputs = layers.Input(shape=input_shape)\n    x = conv_bn_relu(inputs, 64)\n    x = residual_block(x, 64)\n    x = residual_block(x, 128, stride=2)\n    x = residual_block(x, 256, stride=2)\n    x = residual_block(x, 512, stride=2)\n    x = se_block(x)  # optional\n\n    # Advanced pooling: average + max\n    gap = layers.GlobalAveragePooling2D()(x)\n    gmp = layers.GlobalMaxPooling2D()(x)\n    x = layers.Concatenate()([gap, gmp])\n\n    x = layers.Dropout(0.3)(x)\n    x = layers.Dense(512, activation='relu')(x)\n    x = layers.Dropout(0.3)(x)\n    outputs = layers.Dense(num_classes, activation='softmax')(x)\n\n    return models.Model(inputs, outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T10:36:12.800068Z","iopub.execute_input":"2025-11-24T10:36:12.800233Z","iopub.status.idle":"2025-11-24T10:36:12.815065Z","shell.execute_reply.started":"2025-11-24T10:36:12.800221Z","shell.execute_reply":"2025-11-24T10:36:12.814349Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"# PART 11 — Learning Rate Schedule (Cosine + Warmup\n\nConcept\n:\n\t•\tStarts small (warmup) → prevents exploding gradients early\n\t•\tSlowly decays following a cosine wave — helps convergence\n\t•\tThink of it as “start slow, go fast, then cool down smoothl","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport math\n\nclass WarmUpCosine(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, base_lr, epochs, steps_per_epoch, warmup_epochs=5, name=None):\n        super().__init__()\n        self.base_lr = tf.convert_to_tensor(base_lr, dtype=tf.float32)\n        self.epochs = int(epochs)\n        self.steps_per_epoch = int(steps_per_epoch)\n        self.warmup_steps = tf.cast(warmup_epochs * self.steps_per_epoch, tf.float32)\n        self.total_steps = tf.cast(self.epochs * self.steps_per_epoch, tf.float32)\n        self.name = name or \"WarmUpCosine\"\n\n    def __call__(self, step):\n        # make sure step is float32 tensor\n        step = tf.cast(step, tf.float32)\n\n        # Warmup LR: linear ramp from 0 -> base_lr over warmup_steps\n        # Avoid division by zero if warmup_steps == 0\n        warmup_steps = tf.maximum(self.warmup_steps, 1.0)\n        warmup_lr = self.base_lr * (step / warmup_steps)\n\n        # Cosine decay part (after warmup)\n        progress = (step - self.warmup_steps) / tf.maximum(1.0, (self.total_steps - self.warmup_steps))\n        # clip progress to [0,1]\n        progress = tf.clip_by_value(progress, 0.0, 1.0)\n        cosine_decay = 0.5 * (1.0 + tf.cos(math.pi * progress))\n        cosine_lr = self.base_lr * cosine_decay\n\n        # If step < warmup_steps -> warmup_lr else cosine_lr\n        lr = tf.where(step < self.warmup_steps, warmup_lr, cosine_lr)\n        # if step > total_steps, keep lr at 0 (optional) — here we keep clipped cosine_lr (already clipped)\n        return lr\n\n    def get_config(self):\n        return {\n            \"base_lr\": float(self.base_lr.numpy()) if tf.executing_eagerly() else float(self.base_lr),\n            \"epochs\": self.epochs,\n            \"steps_per_epoch\": self.steps_per_epoch,\n            \"warmup_epochs\": int(self.warmup_steps.numpy() // self.steps_per_epoch) if tf.executing_eagerly() else int(self.warmup_steps / self.steps_per_epoch),\n            \"name\": self.name\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T10:36:12.815739Z","iopub.execute_input":"2025-11-24T10:36:12.815926Z","iopub.status.idle":"2025-11-24T10:36:12.832380Z","shell.execute_reply.started":"2025-11-24T10:36:12.815912Z","shell.execute_reply":"2025-11-24T10:36:12.831569Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# steps_per_epoch = math.ceil(len(x_train) / BATCH_SIZE)\n# lr_schedule = WarmUpCosine(INITIAL_LR, epochs=EPOCHS, steps_per_epoch=steps_per_epoch, warmup_epochs=WARMUP_EPOCHS)\n\n# optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=MOMENTUM, nesterov=True)\n\n# # If you use categorical one-hot labels:\n# # loss_fn = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n# # otherwise for integer labels:\n# # loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T10:36:54.136591Z","iopub.execute_input":"2025-11-24T10:36:54.136913Z","iopub.status.idle":"2025-11-24T10:36:54.140324Z","shell.execute_reply.started":"2025-11-24T10:36:54.136888Z","shell.execute_reply":"2025-11-24T10:36:54.139563Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"# PART 12 — Compile the Mode\n\nConcept\n:\n\t•\tSGD + Momentum → stable, proven optimizer for CNNs\n\t•\tLabel smoothing → prevents model from becoming overconfident\n\t•\tCompile → tie the model, loss, and optimizer together before trainin","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.datasets import cifar100\n\n(x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine')\n\ny_train = y_train.squeeze()\ny_test = y_test.squeeze()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T10:36:54.141972Z","iopub.execute_input":"2025-11-24T10:36:54.142218Z","iopub.status.idle":"2025-11-24T10:36:56.506135Z","shell.execute_reply.started":"2025-11-24T10:36:54.142195Z","shell.execute_reply":"2025-11-24T10:36:56.505495Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"steps_per_epoch = len(x_train) // BATCH_SIZE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T10:36:56.506777Z","iopub.execute_input":"2025-11-24T10:36:56.506979Z","iopub.status.idle":"2025-11-24T10:36:56.510578Z","shell.execute_reply.started":"2025-11-24T10:36:56.506963Z","shell.execute_reply":"2025-11-24T10:36:56.509814Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"def se_block(x, se_ratio=8):\n    filters = x.shape[-1]\n    se = tf.keras.layers.GlobalAveragePooling2D()(x)\n    se = tf.keras.layers.Dense(filters // se_ratio, activation='relu')(se)\n    se = tf.keras.layers.Dense(filters, activation='sigmoid')(se)\n    se = tf.keras.layers.Reshape((1,1,filters))(se)\n    return tf.keras.layers.multiply([x, se])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T10:36:56.511429Z","iopub.execute_input":"2025-11-24T10:36:56.511587Z","iopub.status.idle":"2025-11-24T10:36:57.189311Z","shell.execute_reply.started":"2025-11-24T10:36:56.511575Z","shell.execute_reply":"2025-11-24T10:36:57.188443Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"steps_per_epoch = len(x_train) // BATCH_SIZE\nlr_schedule = WarmUpCosine(INITIAL_LR, EPOCHS, steps_per_epoch)\n\noptimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=MOMENTUM, nesterov=True)\nloss_fn = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n\nmodel = build_custom_cnn()\nmodel.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T10:36:57.192110Z","iopub.execute_input":"2025-11-24T10:36:57.192353Z","iopub.status.idle":"2025-11-24T10:36:58.628592Z","shell.execute_reply.started":"2025-11-24T10:36:57.192328Z","shell.execute_reply":"2025-11-24T10:36:58.627780Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"# PART 13 — Callback\n\nConcept:\n\t•\tCheckpoint: saves your best weights\n\t•\tReduceLROnPlateau: lowers LR if loss stops improving\n\t•\tEarlyStopping: stops training early to prevent overfittin","metadata":{}},{"cell_type":"code","source":"callbacks = [\n    tf.keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_accuracy'),\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5),\n    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True)\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T10:36:58.629412Z","iopub.execute_input":"2025-11-24T10:36:58.629748Z","iopub.status.idle":"2025-11-24T10:36:58.633937Z","shell.execute_reply.started":"2025-11-24T10:36:58.629726Z","shell.execute_reply":"2025-11-24T10:36:58.633248Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"# Part 14: Train Model\n\nConcept:\n* Each epoch = full pass over train data\n* Validation used To Track Gen\n* Callback Help Tune Automatically While Training","metadata":{}},{"cell_type":"code","source":"AUTOTUNE = tf.data.AUTOTUNE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T10:36:58.634619Z","iopub.execute_input":"2025-11-24T10:36:58.634893Z","iopub.status.idle":"2025-11-24T10:36:58.653698Z","shell.execute_reply.started":"2025-11-24T10:36:58.634863Z","shell.execute_reply":"2025-11-24T10:36:58.653125Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# Use original integer labels y_train, y_test (shape (N,)) — do NOT one-hot encode\ntrain_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\ntrain_ds = train_ds.map(lambda x,y: (tf.cast(x, tf.float32)/255.0, y), num_parallel_calls=AUTOTUNE)\ntrain_ds = train_ds.map(preprocess_train, num_parallel_calls=AUTOTUNE)   # your augment fn must return (image, int_label)\ntrain_ds = train_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n\ntest_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\ntest_ds = test_ds.map(lambda x,y: (tf.cast(x, tf.float32)/255.0, y), num_parallel_calls=AUTOTUNE)\ntest_ds = test_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n\n# Compile with sparse loss (no label smoothing available on older TF)\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\nmodel.compile(optimizer=optimizer, loss=loss_fn, metrics=['sparse_categorical_accuracy'])\n# or metrics=['accuracy'] — Keras will infer correct accuracy for sparse targets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T10:36:58.654402Z","iopub.execute_input":"2025-11-24T10:36:58.654627Z","iopub.status.idle":"2025-11-24T10:36:59.592172Z","shell.execute_reply.started":"2025-11-24T10:36:58.654604Z","shell.execute_reply":"2025-11-24T10:36:59.591317Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# 1) Create optimizer with a float LR\noptimizer = tf.keras.optimizers.SGD(learning_rate=INITIAL_LR, momentum=MOMENTUM, nesterov=True)\n\n# 2) Define an epoch-based LR function (returns lr for given epoch)\ndef epoch_warmup_cosine(epoch):\n    # simple epoch-based schedule: linear warmup -> cosine decay across EPOCHS\n    warmup = WARMUP_EPOCHS\n    if epoch < warmup:\n        return float(INITIAL_LR * (epoch + 1) / warmup)  # +1 so epoch0 != 0\n    # cosine decay after warmup\n    progress = (epoch - warmup) / max(1, (EPOCHS - warmup))\n    cosine = 0.5 * (1.0 + math.cos(math.pi * min(1.0, progress)))\n    return float(INITIAL_LR * cosine)\n\n# 3) Use LearningRateScheduler (Keras will set optimizer.lr)\nlr_callback = tf.keras.callbacks.LearningRateScheduler(epoch_warmup_cosine, verbose=1)\n\n# 4) Use ReduceLROnPlateau as well (it will set lr when plateau occurs)\nreduce_on_plateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.5, patience=6, verbose=1, mode='min'\n)\n\ncallbacks = [\n    tf.keras.callbacks.ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1),\n    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True, mode='max', verbose=1),\n    lr_callback,\n    reduce_on_plateau\n]\n\nmodel.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T10:36:59.593009Z","iopub.execute_input":"2025-11-24T10:36:59.593679Z","iopub.status.idle":"2025-11-24T10:36:59.604140Z","shell.execute_reply.started":"2025-11-24T10:36:59.593653Z","shell.execute_reply":"2025-11-24T10:36:59.603408Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"history = model.fit(\n    train_ds,\n    validation_data=test_ds,\n    epochs=EPOCHS,\n    callbacks=callbacks\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T11:01:31.132142Z","iopub.execute_input":"2025-11-24T11:01:31.132662Z","iopub.status.idle":"2025-11-24T11:13:58.016312Z","shell.execute_reply.started":"2025-11-24T11:01:31.132639Z","shell.execute_reply":"2025-11-24T11:13:58.015704Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1: LearningRateScheduler setting learning rate to 0.02.\nEpoch 1/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.0079 - loss: 4.6074\nEpoch 1: val_accuracy did not improve from 0.01300\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 65ms/step - accuracy: 0.0079 - loss: 4.6074 - val_accuracy: 0.0127 - val_loss: 4.6540 - learning_rate: 0.0200\n\nEpoch 2: LearningRateScheduler setting learning rate to 0.04.\nEpoch 2/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.0084 - loss: 4.6067\nEpoch 2: val_accuracy did not improve from 0.01300\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 66ms/step - accuracy: 0.0084 - loss: 4.6067 - val_accuracy: 0.0119 - val_loss: 4.6283 - learning_rate: 0.0400\n\nEpoch 3: LearningRateScheduler setting learning rate to 0.06000000000000001.\nEpoch 3/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.0090 - loss: 4.6068\nEpoch 3: val_accuracy did not improve from 0.01300\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 64ms/step - accuracy: 0.0090 - loss: 4.6068 - val_accuracy: 0.0120 - val_loss: 4.6887 - learning_rate: 0.0600\n\nEpoch 4: LearningRateScheduler setting learning rate to 0.08.\nEpoch 4/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.0088 - loss: 4.6074\nEpoch 4: val_accuracy did not improve from 0.01300\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 64ms/step - accuracy: 0.0088 - loss: 4.6074 - val_accuracy: 0.0099 - val_loss: 4.9536 - learning_rate: 0.0800\n\nEpoch 5: LearningRateScheduler setting learning rate to 0.1.\nEpoch 5/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.0089 - loss: 4.6078\nEpoch 5: val_accuracy did not improve from 0.01300\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 64ms/step - accuracy: 0.0089 - loss: 4.6078 - val_accuracy: 0.0088 - val_loss: 4.6724 - learning_rate: 0.1000\n\nEpoch 6: LearningRateScheduler setting learning rate to 0.1.\nEpoch 6/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.0085 - loss: 4.6081\nEpoch 6: val_accuracy did not improve from 0.01300\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 64ms/step - accuracy: 0.0085 - loss: 4.6081 - val_accuracy: 0.0073 - val_loss: 4.7301 - learning_rate: 0.1000\n\nEpoch 7: LearningRateScheduler setting learning rate to 0.09999351124856874.\nEpoch 7/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.0085 - loss: 4.6081\nEpoch 7: val_accuracy did not improve from 0.01300\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 64ms/step - accuracy: 0.0085 - loss: 4.6081 - val_accuracy: 0.0091 - val_loss: 4.6118 - learning_rate: 0.1000\n\nEpoch 8: LearningRateScheduler setting learning rate to 0.09997404667843075.\nEpoch 8/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.0085 - loss: 4.6080\nEpoch 8: val_accuracy did not improve from 0.01300\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 64ms/step - accuracy: 0.0085 - loss: 4.6080 - val_accuracy: 0.0097 - val_loss: 4.6057 - learning_rate: 0.1000\n\nEpoch 9: LearningRateScheduler setting learning rate to 0.09994161134161633.\nEpoch 9/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.0088 - loss: 4.6080\nEpoch 9: val_accuracy did not improve from 0.01300\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 64ms/step - accuracy: 0.0088 - loss: 4.6080 - val_accuracy: 0.0085 - val_loss: 4.9166 - learning_rate: 0.0999\n\nEpoch 10: LearningRateScheduler setting learning rate to 0.09989621365671902.\nEpoch 10/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.0089 - loss: 4.6081\nEpoch 10: val_accuracy improved from 0.01300 to 0.01390, saving model to best_model.h5\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 65ms/step - accuracy: 0.0089 - loss: 4.6081 - val_accuracy: 0.0139 - val_loss: 4.6399 - learning_rate: 0.0999\n\nEpoch 11: LearningRateScheduler setting learning rate to 0.09983786540671051.\nEpoch 11/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.0085 - loss: 4.6080\nEpoch 11: val_accuracy did not improve from 0.01390\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 64ms/step - accuracy: 0.0085 - loss: 4.6080 - val_accuracy: 0.0088 - val_loss: 5.2331 - learning_rate: 0.0998\n\nEpoch 12: LearningRateScheduler setting learning rate to 0.09976658173588243.\nEpoch 12/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.0083 - loss: 4.6080\nEpoch 12: val_accuracy did not improve from 0.01390\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 64ms/step - accuracy: 0.0083 - loss: 4.6080 - val_accuracy: 0.0081 - val_loss: 4.6264 - learning_rate: 0.0998\n\nEpoch 13: LearningRateScheduler setting learning rate to 0.09968238114591566.\nEpoch 13/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.0084 - loss: 4.6080\nEpoch 13: val_accuracy did not improve from 0.01390\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 64ms/step - accuracy: 0.0084 - loss: 4.6080 - val_accuracy: 0.0101 - val_loss: 4.6304 - learning_rate: 0.0997\n\nEpoch 14: LearningRateScheduler setting learning rate to 0.0995852854910781.\nEpoch 14/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.0084 - loss: 4.6080\nEpoch 14: val_accuracy did not improve from 0.01390\n\nEpoch 14: ReduceLROnPlateau reducing learning rate to 0.04979264363646507.\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 63ms/step - accuracy: 0.0084 - loss: 4.6080 - val_accuracy: 0.0114 - val_loss: 4.6457 - learning_rate: 0.0996\n\nEpoch 15: LearningRateScheduler setting learning rate to 0.09947531997255256.\nEpoch 15/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.0084 - loss: 4.6080\nEpoch 15: val_accuracy did not improve from 0.01390\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 63ms/step - accuracy: 0.0084 - loss: 4.6080 - val_accuracy: 0.0077 - val_loss: 4.7492 - learning_rate: 0.0995\n\nEpoch 16: LearningRateScheduler setting learning rate to 0.09935251313189564.\nEpoch 16/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.0084 - loss: 4.6080\nEpoch 16: val_accuracy did not improve from 0.01390\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 63ms/step - accuracy: 0.0084 - loss: 4.6080 - val_accuracy: 0.0107 - val_loss: 4.8479 - learning_rate: 0.0994\n\nEpoch 17: LearningRateScheduler setting learning rate to 0.09921689684362989.\nEpoch 17/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.0084 - loss: 4.6080\nEpoch 17: val_accuracy did not improve from 0.01390\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 63ms/step - accuracy: 0.0084 - loss: 4.6080 - val_accuracy: 0.0107 - val_loss: 4.9404 - learning_rate: 0.0992\n\nEpoch 18: LearningRateScheduler setting learning rate to 0.09906850630697067.\nEpoch 18/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.0084 - loss: 4.6080\nEpoch 18: val_accuracy did not improve from 0.01390\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 63ms/step - accuracy: 0.0084 - loss: 4.6080 - val_accuracy: 0.0112 - val_loss: 4.9576 - learning_rate: 0.0991\n\nEpoch 19: LearningRateScheduler setting learning rate to 0.09890738003669029.\nEpoch 19/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.0085 - loss: 4.6080\nEpoch 19: val_accuracy did not improve from 0.01390\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 64ms/step - accuracy: 0.0085 - loss: 4.6080 - val_accuracy: 0.0109 - val_loss: 4.8994 - learning_rate: 0.0989\n\nEpoch 20: LearningRateScheduler setting learning rate to 0.0987335598531214.\nEpoch 20/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.0085 - loss: 4.6080\nEpoch 20: val_accuracy did not improve from 0.01390\n\nEpoch 20: ReduceLROnPlateau reducing learning rate to 0.0493667796254158.\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 63ms/step - accuracy: 0.0085 - loss: 4.6080 - val_accuracy: 0.0113 - val_loss: 4.9342 - learning_rate: 0.0987\n\nEpoch 21: LearningRateScheduler setting learning rate to 0.0985470908713026.\nEpoch 21/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.0086 - loss: 4.6080\nEpoch 21: val_accuracy did not improve from 0.01390\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 63ms/step - accuracy: 0.0086 - loss: 4.6080 - val_accuracy: 0.0110 - val_loss: 4.9245 - learning_rate: 0.0985\n\nEpoch 22: LearningRateScheduler setting learning rate to 0.09834802148926883.\nEpoch 22/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.0086 - loss: 4.6080\nEpoch 22: val_accuracy did not improve from 0.01390\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 63ms/step - accuracy: 0.0086 - loss: 4.6080 - val_accuracy: 0.0099 - val_loss: 4.9325 - learning_rate: 0.0983\n\nEpoch 23: LearningRateScheduler setting learning rate to 0.09813640337548954.\nEpoch 23/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.0084 - loss: 4.6080\nEpoch 23: val_accuracy did not improve from 0.01390\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 63ms/step - accuracy: 0.0084 - loss: 4.6080 - val_accuracy: 0.0107 - val_loss: 4.8843 - learning_rate: 0.0981\n\nEpoch 24: LearningRateScheduler setting learning rate to 0.09791229145545832.\nEpoch 24/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.0086 - loss: 4.6080\nEpoch 24: val_accuracy did not improve from 0.01390\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 63ms/step - accuracy: 0.0086 - loss: 4.6080 - val_accuracy: 0.0103 - val_loss: 4.8488 - learning_rate: 0.0979\n\nEpoch 25: LearningRateScheduler setting learning rate to 0.09767574389743683.\nEpoch 25/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.0086 - loss: 4.6080\nEpoch 25: val_accuracy did not improve from 0.01390\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 63ms/step - accuracy: 0.0086 - loss: 4.6080 - val_accuracy: 0.0094 - val_loss: 4.7611 - learning_rate: 0.0977\n\nEpoch 26: LearningRateScheduler setting learning rate to 0.09742682209735727.\nEpoch 26/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.0085 - loss: 4.6080\nEpoch 26: val_accuracy did not improve from 0.01390\n\nEpoch 26: ReduceLROnPlateau reducing learning rate to 0.04871341213583946.\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 63ms/step - accuracy: 0.0085 - loss: 4.6080 - val_accuracy: 0.0092 - val_loss: 4.7624 - learning_rate: 0.0974\n\nEpoch 27: LearningRateScheduler setting learning rate to 0.09716559066288716.\nEpoch 27/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.0088 - loss: 4.6080\nEpoch 27: val_accuracy did not improve from 0.01390\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 63ms/step - accuracy: 0.0088 - loss: 4.6080 - val_accuracy: 0.0076 - val_loss: 4.6199 - learning_rate: 0.0972\n\nEpoch 28: LearningRateScheduler setting learning rate to 0.09689211739666023.\nEpoch 28/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.0086 - loss: 4.6080\nEpoch 28: val_accuracy did not improve from 0.01390\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 63ms/step - accuracy: 0.0086 - loss: 4.6080 - val_accuracy: 0.0073 - val_loss: 4.6232 - learning_rate: 0.0969\n\nEpoch 29: LearningRateScheduler setting learning rate to 0.0966064732786784.\nEpoch 29/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.0086 - loss: 4.6079\nEpoch 29: val_accuracy did not improve from 0.01390\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 62ms/step - accuracy: 0.0086 - loss: 4.6079 - val_accuracy: 0.0074 - val_loss: 4.6224 - learning_rate: 0.0966\n\nEpoch 30: LearningRateScheduler setting learning rate to 0.09630873244788883.\nEpoch 30/200\n\u001b[1m390/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.0086 - loss: 4.6079\nEpoch 30: val_accuracy did not improve from 0.01390\n\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 63ms/step - accuracy: 0.0086 - loss: 4.6079 - val_accuracy: 0.0080 - val_loss: 4.6503 - learning_rate: 0.0963\nEpoch 30: early stopping\nRestoring model weights from the end of the best epoch: 10.\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"# PART 15 — Evaluate and Visualize\n\nConcept:\n\n\t•\tLoad best model checkpoint\n\t•\tEvaluate on unseen test set\n\t•\tExpect accuracy to gradually climb toward 60–65% with good tunin","metadata":{}},{"cell_type":"code","source":"model.load_weights('best_model.h5')\ntest_loss, test_acc = model.evaluate(test_ds)\nprint(\"Test Accuracy:\", test_acc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T11:00:30.065729Z","iopub.execute_input":"2025-11-24T11:00:30.066623Z","iopub.status.idle":"2025-11-24T11:00:32.714184Z","shell.execute_reply.started":"2025-11-24T11:00:30.066597Z","shell.execute_reply":"2025-11-24T11:00:32.713356Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.0134 - loss: 4.7673\nTest Accuracy: 0.013000000268220901\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}