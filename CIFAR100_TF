{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-26T04:04:39.159816Z","iopub.execute_input":"2025-11-26T04:04:39.160089Z","iopub.status.idle":"2025-11-26T04:04:40.784097Z","shell.execute_reply.started":"2025-11-26T04:04:39.160063Z","shell.execute_reply":"2025-11-26T04:04:40.783279Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# CIFAR\n\n**Project Overview**\n\nGoal:\n\nBuild a Custom CNN (no transfer learning) that can classify CIFAR-100 images into 100 classes with ~65% accuracy.\n\nDataset:\n\n\t•\tCIFAR-100: 50k train + 10k test, each image is 32×32 RGB, 100 classes.\n\t•\tExample classes: apple, chair, shark, pickup truck, leopard, etc.\n\nApproach:\n\t\n    1.\tLoad dataset & prepare data pipeline\n\t2.\tCreate augmentations\n\t3.\tBuild CNN with residual blocks, BatchNorm, Dropout\n\t4.\tTrain with smart learning rate schedule\n\t5.\tEvaluate performance\n**","metadata":{}},{"cell_type":"code","source":"# # imports\n\n# import tensorflow as tf\n# import numpy as np\n# import math","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T04:05:05.863224Z","iopub.execute_input":"2025-11-26T04:05:05.863802Z","iopub.status.idle":"2025-11-26T04:05:05.867159Z","shell.execute_reply.started":"2025-11-26T04:05:05.863774Z","shell.execute_reply":"2025-11-26T04:05:05.866444Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# import pickle\n# import numpy as np\n\n# def load_cifar100(path):\n#     with open(path, 'rb') as f:\n#         data = pickle.load(f, encoding='latin1')\n#     X = data['data'].reshape(-1, 3, 32, 32).transpose(0,2,3,1)\n#     y = np.array(data['fine_labels'])\n#     return X, y\n\n# x_train, y_train = load_cifar100('/kaggle/input/cifar100/train')\n# x_test, y_test   = load_cifar100('/kaggle/input/cifar100/test')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T04:05:06.299659Z","iopub.execute_input":"2025-11-26T04:05:06.299932Z","iopub.status.idle":"2025-11-26T04:05:06.303790Z","shell.execute_reply.started":"2025-11-26T04:05:06.299911Z","shell.execute_reply":"2025-11-26T04:05:06.302813Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Full CIFAR-100 training script (Kaggle-ready, single cell)\n# - Uses tf.keras.datasets.cifar100\n# - Uses ResNet152V2 pretrained on ImageNet (top excluded)\n# - Resizes CIFAR images to 224x224, uses preprocess_input\n# - Uses ImageDataGenerator for augmentation\n# - Checkpointing, EarlyStopping, evaluation, saving (modern .keras)\n# Copy-paste this entire cell into Kaggle and run.\n\nimport os\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, regularizers, optimizers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.applications.resnet_v2 import ResNet152V2, preprocess_input\nfrom tensorflow.keras import backend as K\ntf.random.set_seed(42)\nnp.random.seed(42)\n\n# ---------- Config ----------\nBATCH_SIZE = 64\nEPOCHS = 20          # bump up later (e.g., 50-150) for final training\nIMG_SIZE = (224, 224)  # ResNet expected size\nNUM_CLASSES = 100\nCHECKPOINT_PATH = '/kaggle/working/best_model.keras'\nFINAL_MODEL_PATH = '/kaggle/working/final_model.keras'\n\n# ---------- Load CIFAR-100 ----------\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar100.load_data(label_mode='fine')\n# x_*: uint8 in [0,255], shape (N,32,32,3)\n# y_*: shape (N,1) with integer labels 0..99\ny_train = y_train.flatten()\ny_test = y_test.flatten()\n\n# Create a validation split from training set\nVAL_SIZE = 10000\nx_val = x_train[-VAL_SIZE:]\ny_val = y_train[-VAL_SIZE:]\nx_train = x_train[:-VAL_SIZE]\ny_train = y_train[:-VAL_SIZE]\n\nprint(\"Shapes:\", x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape)\n\n# ---------- Data generators ----------\n# Use preprocess_input from ResNetV2 which expects inputs in range [0,255] and performs model-specific scaling.\ntrain_datagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,\n    rotation_range=10,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    horizontal_flip=True,\n    fill_mode='reflect'\n)\n\nval_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n\n# flow() will handle resizing via a custom generator below (we need to resize images to 224x224)\ndef generator_from_arrays(datagen, x_array, y_array, batch_size, shuffle=True):\n    \"\"\"Yield batches resized to IMG_SIZE and preprocessed by datagen.\"\"\"\n    n = len(x_array)\n    idxs = np.arange(n)\n    while True:\n        if shuffle:\n            np.random.shuffle(idxs)\n        for i in range(0, n, batch_size):\n            batch_idx = idxs[i:i+batch_size]\n            batch_x = x_array[batch_idx].astype('float32')  # still 0-255\n            # resize to IMG_SIZE\n            batch_x_resized = np.zeros((len(batch_x), IMG_SIZE[0], IMG_SIZE[1], 3), dtype='float32')\n            for j in range(len(batch_x)):\n                img = tf.image.resize(batch_x[j], IMG_SIZE, method='bilinear').numpy()\n                batch_x_resized[j] = img\n            # let datagen apply preprocess_input and augmentation\n            batch_gen = datagen.flow(batch_x_resized, y_array[batch_idx], batch_size=len(batch_x_resized), shuffle=False)\n            xb, yb = next(batch_gen)\n            yield xb, yb\n\ntrain_steps = math.ceil(len(x_train) / BATCH_SIZE)\nval_steps   = math.ceil(len(x_val)   / BATCH_SIZE)\ntest_steps  = math.ceil(len(x_test)  / BATCH_SIZE)\n\ntrain_gen = generator_from_arrays(train_datagen, x_train, y_train, BATCH_SIZE, shuffle=True)\nval_gen   = generator_from_arrays(val_datagen,   x_val,   y_val,   BATCH_SIZE, shuffle=False)\ntest_gen  = generator_from_arrays(val_datagen,   x_test,  y_test,  BATCH_SIZE, shuffle=False)\n\n# ---------- Build model (transfer learning) ----------\n# Base model: ResNet152V2 without top\nbase_model = ResNet152V2(weights='imagenet', include_top=False, input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n# Freeze all convolutional layers initially, but keep BatchNorm trainable for stability\nfor layer in base_model.layers:\n    if isinstance(layer, tf.keras.layers.BatchNormalization):\n        layer.trainable = True\n    else:\n        layer.trainable = False\n\ninputs = layers.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\nx = base_model(inputs, training=False)\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.Dropout(0.3)(x)\nx = layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(x)\nx = layers.BatchNormalization()(x)\nx = layers.Dropout(0.3)(x)\noutputs = layers.Dense(NUM_CLASSES, activation='softmax')(x)\nmodel = models.Model(inputs, outputs, name='resnet152v2_cifar100')\n\nmodel.summary()\n\n# ---------- Compile ----------\n# Use a small lr with Adam for fine-tuning; you can switch to SGD later\noptimizer = optimizers.Adam(learning_rate=1e-4)\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\nmodel.compile(optimizer=optimizer, loss=loss_fn, metrics=['sparse_categorical_accuracy'])\n\n# ---------- Callbacks ----------\ncallbacks = [\n    ModelCheckpoint(CHECKPOINT_PATH, monitor='val_sparse_categorical_accuracy', save_best_only=True, mode='max', verbose=1),\n    EarlyStopping(monitor='val_sparse_categorical_accuracy', patience=8, restore_best_weights=True, mode='max', verbose=1),\n    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1, mode='min')\n]\n\n# ---------- Train ----------\nprint(\"Starting training — steps per epoch:\", train_steps, \"val steps:\", val_steps)\nhistory = model.fit(\n    train_gen,\n    steps_per_epoch=train_steps,\n    validation_data=val_gen,\n    validation_steps=val_steps,\n    epochs=EPOCHS,\n    callbacks=callbacks,\n    verbose=2\n)\n\n# ---------- Evaluate on test set ----------\nprint(\"\\nEvaluating on test set...\")\ntest_loss, test_acc = model.evaluate(test_gen, steps=test_steps, verbose=2)\nprint(\"Test loss:\", test_loss)\nprint(\"Test sparse accuracy:\", test_acc)\n\n# ---------- Save final model ----------\nmodel.save(FINAL_MODEL_PATH)\nprint(\"Saved final model to:\", FINAL_MODEL_PATH)\n\n# ---------- Plot training curves ----------\nplt.figure(figsize=(12,4))\nplt.subplot(1,2,1)\nplt.plot(history.history['loss'], label='train loss')\nplt.plot(history.history['val_loss'], label='val loss')\nplt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T04:09:42.565536Z","iopub.execute_input":"2025-11-26T04:09:42.566178Z","iopub.status.idle":"2025-11-26T09:41:48.904699Z","shell.execute_reply.started":"2025-11-26T04:09:42.566149Z","shell.execute_reply":"2025-11-26T09:41:48.904096Z"}},"outputs":[{"name":"stderr","text":"2025-11-26 04:09:44.172163: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764130184.388108      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764130184.446850      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n\u001b[1m169001437/169001437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\nShapes: (40000, 32, 32, 3) (40000,) (10000, 32, 32, 3) (10000,) (10000, 32, 32, 3) (10000,)\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1764130205.920902      47 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1764130205.921542      47 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet152v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m234545216/234545216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"resnet152v2_cifar100\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"resnet152v2_cifar100\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ resnet152v2 (\u001b[38;5;33mFunctional\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │    \u001b[38;5;34m58,331,648\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m1,049,088\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m51,300\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ resnet152v2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">58,331,648</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,088</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">51,300</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m59,434,084\u001b[0m (226.72 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">59,434,084</span> (226.72 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,245,156\u001b[0m (4.75 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,245,156</span> (4.75 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m58,188,928\u001b[0m (221.97 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,188,928</span> (221.97 MB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Starting training — steps per epoch: 625 val steps: 157\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1764130270.660340     111 service.cc:148] XLA service 0x7d8e700042a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1764130270.661529     111 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1764130270.661549     111 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1764130279.685304     111 cuda_dnn.cc:529] Loaded cuDNN version 90300\nI0000 00:00:1764130311.269697     111 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1: val_sparse_categorical_accuracy improved from -inf to 0.57570, saving model to /kaggle/working/best_model.keras\n625/625 - 1133s - 2s/step - loss: 3.3834 - sparse_categorical_accuracy: 0.2607 - val_loss: 1.6738 - val_sparse_categorical_accuracy: 0.5757 - learning_rate: 1.0000e-04\nEpoch 2/20\n\nEpoch 2: val_sparse_categorical_accuracy improved from 0.57570 to 0.66100, saving model to /kaggle/working/best_model.keras\n625/625 - 1023s - 2s/step - loss: 1.9969 - sparse_categorical_accuracy: 0.4988 - val_loss: 1.2962 - val_sparse_categorical_accuracy: 0.6610 - learning_rate: 1.0000e-04\nEpoch 3/20\n\nEpoch 3: val_sparse_categorical_accuracy improved from 0.66100 to 0.69470, saving model to /kaggle/working/best_model.keras\n625/625 - 1024s - 2s/step - loss: 1.6344 - sparse_categorical_accuracy: 0.5744 - val_loss: 1.1380 - val_sparse_categorical_accuracy: 0.6947 - learning_rate: 1.0000e-04\nEpoch 4/20\n\nEpoch 4: val_sparse_categorical_accuracy improved from 0.69470 to 0.71970, saving model to /kaggle/working/best_model.keras\n625/625 - 1017s - 2s/step - loss: 1.4452 - sparse_categorical_accuracy: 0.6180 - val_loss: 1.0432 - val_sparse_categorical_accuracy: 0.7197 - learning_rate: 1.0000e-04\nEpoch 5/20\n\nEpoch 5: val_sparse_categorical_accuracy improved from 0.71970 to 0.73350, saving model to /kaggle/working/best_model.keras\n625/625 - 1012s - 2s/step - loss: 1.3041 - sparse_categorical_accuracy: 0.6493 - val_loss: 0.9825 - val_sparse_categorical_accuracy: 0.7335 - learning_rate: 1.0000e-04\nEpoch 6/20\n\nEpoch 6: val_sparse_categorical_accuracy improved from 0.73350 to 0.74820, saving model to /kaggle/working/best_model.keras\n625/625 - 1001s - 2s/step - loss: 1.2032 - sparse_categorical_accuracy: 0.6755 - val_loss: 0.9351 - val_sparse_categorical_accuracy: 0.7482 - learning_rate: 1.0000e-04\nEpoch 7/20\n\nEpoch 7: val_sparse_categorical_accuracy improved from 0.74820 to 0.75540, saving model to /kaggle/working/best_model.keras\n625/625 - 975s - 2s/step - loss: 1.1313 - sparse_categorical_accuracy: 0.6914 - val_loss: 0.9004 - val_sparse_categorical_accuracy: 0.7554 - learning_rate: 1.0000e-04\nEpoch 8/20\n\nEpoch 8: val_sparse_categorical_accuracy improved from 0.75540 to 0.76130, saving model to /kaggle/working/best_model.keras\n625/625 - 973s - 2s/step - loss: 1.0672 - sparse_categorical_accuracy: 0.7076 - val_loss: 0.8699 - val_sparse_categorical_accuracy: 0.7613 - learning_rate: 1.0000e-04\nEpoch 9/20\n\nEpoch 9: val_sparse_categorical_accuracy improved from 0.76130 to 0.77010, saving model to /kaggle/working/best_model.keras\n625/625 - 970s - 2s/step - loss: 1.0091 - sparse_categorical_accuracy: 0.7203 - val_loss: 0.8458 - val_sparse_categorical_accuracy: 0.7701 - learning_rate: 1.0000e-04\nEpoch 10/20\n\nEpoch 10: val_sparse_categorical_accuracy improved from 0.77010 to 0.77500, saving model to /kaggle/working/best_model.keras\n625/625 - 968s - 2s/step - loss: 0.9785 - sparse_categorical_accuracy: 0.7298 - val_loss: 0.8284 - val_sparse_categorical_accuracy: 0.7750 - learning_rate: 1.0000e-04\nEpoch 11/20\n\nEpoch 11: val_sparse_categorical_accuracy improved from 0.77500 to 0.77960, saving model to /kaggle/working/best_model.keras\n625/625 - 972s - 2s/step - loss: 0.9208 - sparse_categorical_accuracy: 0.7464 - val_loss: 0.8108 - val_sparse_categorical_accuracy: 0.7796 - learning_rate: 1.0000e-04\nEpoch 12/20\n\nEpoch 12: val_sparse_categorical_accuracy improved from 0.77960 to 0.78260, saving model to /kaggle/working/best_model.keras\n625/625 - 974s - 2s/step - loss: 0.8827 - sparse_categorical_accuracy: 0.7562 - val_loss: 0.7954 - val_sparse_categorical_accuracy: 0.7826 - learning_rate: 1.0000e-04\nEpoch 13/20\n\nEpoch 13: val_sparse_categorical_accuracy improved from 0.78260 to 0.78770, saving model to /kaggle/working/best_model.keras\n625/625 - 974s - 2s/step - loss: 0.8601 - sparse_categorical_accuracy: 0.7624 - val_loss: 0.7801 - val_sparse_categorical_accuracy: 0.7877 - learning_rate: 1.0000e-04\nEpoch 14/20\n\nEpoch 14: val_sparse_categorical_accuracy improved from 0.78770 to 0.79130, saving model to /kaggle/working/best_model.keras\n625/625 - 969s - 2s/step - loss: 0.8323 - sparse_categorical_accuracy: 0.7689 - val_loss: 0.7650 - val_sparse_categorical_accuracy: 0.7913 - learning_rate: 1.0000e-04\nEpoch 15/20\n\nEpoch 15: val_sparse_categorical_accuracy improved from 0.79130 to 0.79640, saving model to /kaggle/working/best_model.keras\n625/625 - 971s - 2s/step - loss: 0.8041 - sparse_categorical_accuracy: 0.7773 - val_loss: 0.7558 - val_sparse_categorical_accuracy: 0.7964 - learning_rate: 1.0000e-04\nEpoch 16/20\n\nEpoch 16: val_sparse_categorical_accuracy improved from 0.79640 to 0.79740, saving model to /kaggle/working/best_model.keras\n625/625 - 968s - 2s/step - loss: 0.7848 - sparse_categorical_accuracy: 0.7807 - val_loss: 0.7465 - val_sparse_categorical_accuracy: 0.7974 - learning_rate: 1.0000e-04\nEpoch 17/20\n\nEpoch 17: val_sparse_categorical_accuracy improved from 0.79740 to 0.80090, saving model to /kaggle/working/best_model.keras\n625/625 - 976s - 2s/step - loss: 0.7641 - sparse_categorical_accuracy: 0.7886 - val_loss: 0.7390 - val_sparse_categorical_accuracy: 0.8009 - learning_rate: 1.0000e-04\nEpoch 18/20\n\nEpoch 18: val_sparse_categorical_accuracy improved from 0.80090 to 0.80230, saving model to /kaggle/working/best_model.keras\n625/625 - 970s - 2s/step - loss: 0.7487 - sparse_categorical_accuracy: 0.7915 - val_loss: 0.7329 - val_sparse_categorical_accuracy: 0.8023 - learning_rate: 1.0000e-04\nEpoch 19/20\n\nEpoch 19: val_sparse_categorical_accuracy improved from 0.80230 to 0.80340, saving model to /kaggle/working/best_model.keras\n625/625 - 972s - 2s/step - loss: 0.7275 - sparse_categorical_accuracy: 0.7950 - val_loss: 0.7238 - val_sparse_categorical_accuracy: 0.8034 - learning_rate: 1.0000e-04\nEpoch 20/20\n\nEpoch 20: val_sparse_categorical_accuracy improved from 0.80340 to 0.80510, saving model to /kaggle/working/best_model.keras\n625/625 - 972s - 2s/step - loss: 0.7088 - sparse_categorical_accuracy: 0.8026 - val_loss: 0.7221 - val_sparse_categorical_accuracy: 0.8051 - learning_rate: 1.0000e-04\nRestoring model weights from the end of the best epoch: 20.\n\nEvaluating on test set...\n157/157 - 78s - 497ms/step - loss: 0.6982 - sparse_categorical_accuracy: 0.8112\nTest loss: 0.6982496976852417\nTest sparse accuracy: 0.8112000226974487\nSaved final model to: /kaggle/working/final_model.keras\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"<module 'matplotlib.pyplot' from '/usr/local/lib/python3.11/dist-packages/matplotlib/pyplot.py'>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x400 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"What’s happening:\n\n\t•\ttensorflow → our deep learning framework\n\t•\tlayers, models, regularizers → Keras tools for building CNNs\n    \n\t•\tnumpy → for array math\n\t•\tmath → for cosine learning rate scheduling","metadata":{}},{"cell_type":"markdown","source":"# PART 3 — Hyperparameter","metadata":{}},{"cell_type":"markdown","source":"Concept:\n\nThink of these like dials on a control board:\n\n\t•\tBATCH_SIZE: number of images per step\n\t•\tEPOCHS: how many passes over the training data\n\t•\tMIXUP_ALPHA: controls MixUp augmentation (image blending)\n\t•\tLABEL_SMOOTHING: prevents overconfidence in predictions\n\t•\tWEIGHT_DECAY: small penalty on weights to avoid                   overfitting\n    \n\t•\tLR (Learning Rate): how fast the model learns\n\t•\tMOMENTUM: helps SGD optimizer “remember” direction\n\t•\tWARMUP: slowly increase LR early to stabilize training","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 128\nIMG_SHAPE = (32, 32, 3)\nNUM_CLASSES = 100\nEPOCHS = 200\n\nMIXUP_ALPHA = 0.2\nLABEL_SMOOTHING = 0.1\nWEIGHT_DECAY = 1e-4\n\nINITIAL_LR = 0.1\nMOMENTUM = 0.9\nWARMUP_EPOCHS = 5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T01:11:46.486382Z","iopub.execute_input":"2025-11-25T01:11:46.486628Z","iopub.status.idle":"2025-11-25T01:11:46.490943Z","shell.execute_reply.started":"2025-11-25T01:11:46.486609Z","shell.execute_reply":"2025-11-25T01:11:46.490313Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# PART 4 — Data Loading\n\nConcept:\n\n\t•\tTensorFlow gives you CIFAR-100 ready to go.\n\t•\tEach x = images, y = class labels.\n\t•\tlabel_mode='fine' → 100 detailed classes (not grouped into 20 superclasses).\n\t•\ttf.squeeze removes unnecessary shape dimensions","metadata":{}},{"cell_type":"code","source":"(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar100.load_data(label_mode = 'fine')\ny_train = tf.squeeze(y_train)\ny_test = tf.squeeze(y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T01:11:46.491631Z","iopub.execute_input":"2025-11-25T01:11:46.491916Z","iopub.status.idle":"2025-11-25T01:11:51.787523Z","shell.execute_reply.started":"2025-11-25T01:11:46.491898Z","shell.execute_reply":"2025-11-25T01:11:51.786762Z"}},"outputs":[{"name":"stdout","text":"Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n\u001b[1m169001437/169001437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1764033111.757829      47 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1764033111.758400      47 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# PART 5 — Data Preprocessing (Train vs Test)\n\nConcept:\n\n\t•\tNormalization: divide by 255 to get pixels between [0,1]\n\t•\tAugmentation (train only): random crop, flip, brightness — makes model more robust\n\t•\tNo augmentation on test data, only normalize","metadata":{}},{"cell_type":"code","source":"def preprocess_train(image, label):\n    image = tf.cast(image, tf.float32) / 255.0\n    image = tf.image.resize_with_crop_or_pad(image, 36, 36)\n    image = tf.image.random_crop(image, [32, 32, 3])\n    image = tf.image.random_flip_left_right(image)\n    return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T01:11:51.788401Z","iopub.execute_input":"2025-11-25T01:11:51.788681Z","iopub.status.idle":"2025-11-25T01:11:51.793691Z","shell.execute_reply.started":"2025-11-25T01:11:51.788652Z","shell.execute_reply":"2025-11-25T01:11:51.793045Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Eval Prep\ndef preprocess_eval(image, label):\n    return tf.cast(image, tf.float32) / 255.0, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T01:11:51.796008Z","iopub.execute_input":"2025-11-25T01:11:51.796255Z","iopub.status.idle":"2025-11-25T01:11:51.803591Z","shell.execute_reply.started":"2025-11-25T01:11:51.796238Z","shell.execute_reply":"2025-11-25T01:11:51.802966Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# PART 6 — Build the TensorFlow Dataset Pipeline\n\nConcept:\n\n\t•\ttf.data.Dataset efficiently feeds data to GPU.\n\t•\tshuffle prevents the model from memorizing the order.\n\t•\tmap applies your preprocessing function.\n\t•\tprefetch overlaps data loading with GPU work for speed","metadata":{}},{"cell_type":"code","source":"train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\ntrain_ds = train_ds.shuffle(50000)\ntrain_ds = train_ds.map(preprocess_train, num_parallel_calls = tf.data.AUTOTUNE)\ntrain_ds = train_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\ntest_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))\ntest_ds = test_ds.map(preprocess_eval).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T01:11:51.804188Z","iopub.execute_input":"2025-11-25T01:11:51.804423Z","iopub.status.idle":"2025-11-25T01:11:52.724081Z","shell.execute_reply.started":"2025-11-25T01:11:51.804391Z","shell.execute_reply":"2025-11-25T01:11:52.723520Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# PART 7 — CNN Building Blocks\n\nConcept:\n\n\t•\tConv2D → extracts features (edges, textures, etc.)\n\t•\tBatchNorm → stabilizes and speeds up learning\n\t•\tReLU → adds non-linearity","metadata":{}},{"cell_type":"code","source":"def conv_bn_relu(x, filters, kernel_size=3, stride=1):\n    x = layers.Conv2D(filters, kernel_size, strides=stride, padding='same', use_bias=False)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n    return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T01:11:52.724772Z","iopub.execute_input":"2025-11-25T01:11:52.724947Z","iopub.status.idle":"2025-11-25T01:11:52.729049Z","shell.execute_reply.started":"2025-11-25T01:11:52.724928Z","shell.execute_reply":"2025-11-25T01:11:52.728323Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# PART 8 — Residual Block\n\nConcept:\n\nResidual blocks = ResNet magic\n\nThey skip connections, letting gradients flow easily through deep networks.\n\nYou can think: “learn the change (residual) instead of starting from scratch each layer\"","metadata":{}},{"cell_type":"code","source":"def residual_block(x, filters, stride=1):\n    shortcut = x\n    out = conv_bn_relu(x, filters, 3, stride)\n    out = layers.Conv2D(filters, 3, padding='same', use_bias=False)(out)\n    out = layers.BatchNormalization()(out)\n\n    # adjust shortcut size if needed\n    if stride != 1 or x.shape[-1] != filters:\n        shortcut = layers.Conv2D(filters, 1, strides=stride, use_bias=False)(shortcut)\n        shortcut = layers.BatchNormalization()(shortcut)\n\n    x = layers.add([out, shortcut])\n    x = layers.ReLU()(x)\n    return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T01:11:52.729714Z","iopub.execute_input":"2025-11-25T01:11:52.729952Z","iopub.status.idle":"2025-11-25T01:11:52.740615Z","shell.execute_reply.started":"2025-11-25T01:11:52.729932Z","shell.execute_reply":"2025-11-25T01:11:52.739885Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# PART 10 — Full CNN Architecture\n\nConcept:\n\n\t•\tDeep stacked residual layers → feature extraction\n\t•\tGlobalAveragePooling + GlobalMaxPooling → captures both mean and extreme features\n\t•\tDense layer → final classifier\n\t•\tSoftmax → converts to probabilities","metadata":{}},{"cell_type":"code","source":"def build_custom_cnn(input_shape=(32,32,3), num_classes=100):\n    inputs = layers.Input(shape=input_shape)\n    x = conv_bn_relu(inputs, 64)\n    x = residual_block(x, 64)\n    x = residual_block(x, 128, stride=2)\n    x = residual_block(x, 256, stride=2)\n    x = residual_block(x, 512, stride=2)\n    x = se_block(x)  # optional\n\n    # Advanced pooling: average + max\n    gap = layers.GlobalAveragePooling2D()(x)\n    gmp = layers.GlobalMaxPooling2D()(x)\n    x = layers.Concatenate()([gap, gmp])\n\n    x = layers.Dropout(0.3)(x)\n    x = layers.Dense(512, activation='relu')(x)\n    x = layers.Dropout(0.3)(x)\n    outputs = layers.Dense(num_classes, activation='softmax')(x)\n\n    return models.Model(inputs, outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T01:11:52.741310Z","iopub.execute_input":"2025-11-25T01:11:52.741607Z","iopub.status.idle":"2025-11-25T01:11:52.752538Z","shell.execute_reply.started":"2025-11-25T01:11:52.741590Z","shell.execute_reply":"2025-11-25T01:11:52.751981Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# PART 11 — Learning Rate Schedule (Cosine + Warmup\n\nConcept\n:\n\t•\tStarts small (warmup) → prevents exploding gradients early\n\t•\tSlowly decays following a cosine wave — helps convergence\n\t•\tThink of it as “start slow, go fast, then cool down smoothl","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport math\n\nclass WarmUpCosine(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, base_lr, epochs, steps_per_epoch, warmup_epochs=5, name=None):\n        super().__init__()\n        self.base_lr = tf.convert_to_tensor(base_lr, dtype=tf.float32)\n        self.epochs = int(epochs)\n        self.steps_per_epoch = int(steps_per_epoch)\n        self.warmup_steps = tf.cast(warmup_epochs * self.steps_per_epoch, tf.float32)\n        self.total_steps = tf.cast(self.epochs * self.steps_per_epoch, tf.float32)\n        self.name = name or \"WarmUpCosine\"\n\n    def __call__(self, step):\n        # make sure step is float32 tensor\n        step = tf.cast(step, tf.float32)\n\n        # Warmup LR: linear ramp from 0 -> base_lr over warmup_steps\n        # Avoid division by zero if warmup_steps == 0\n        warmup_steps = tf.maximum(self.warmup_steps, 1.0)\n        warmup_lr = self.base_lr * (step / warmup_steps)\n\n        # Cosine decay part (after warmup)\n        progress = (step - self.warmup_steps) / tf.maximum(1.0, (self.total_steps - self.warmup_steps))\n        # clip progress to [0,1]\n        progress = tf.clip_by_value(progress, 0.0, 1.0)\n        cosine_decay = 0.5 * (1.0 + tf.cos(math.pi * progress))\n        cosine_lr = self.base_lr * cosine_decay\n\n        # If step < warmup_steps -> warmup_lr else cosine_lr\n        lr = tf.where(step < self.warmup_steps, warmup_lr, cosine_lr)\n        # if step > total_steps, keep lr at 0 (optional) — here we keep clipped cosine_lr (already clipped)\n        return lr\n\n    def get_config(self):\n        return {\n            \"base_lr\": float(self.base_lr.numpy()) if tf.executing_eagerly() else float(self.base_lr),\n            \"epochs\": self.epochs,\n            \"steps_per_epoch\": self.steps_per_epoch,\n            \"warmup_epochs\": int(self.warmup_steps.numpy() // self.steps_per_epoch) if tf.executing_eagerly() else int(self.warmup_steps / self.steps_per_epoch),\n            \"name\": self.name\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T01:11:52.753220Z","iopub.execute_input":"2025-11-25T01:11:52.753775Z","iopub.status.idle":"2025-11-25T01:11:52.765412Z","shell.execute_reply.started":"2025-11-25T01:11:52.753753Z","shell.execute_reply":"2025-11-25T01:11:52.764909Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# steps_per_epoch = math.ceil(len(x_train) / BATCH_SIZE)\n# lr_schedule = WarmUpCosine(INITIAL_LR, epochs=EPOCHS, steps_per_epoch=steps_per_epoch, warmup_epochs=WARMUP_EPOCHS)\n\n# optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=MOMENTUM, nesterov=True)\n\n# # If you use categorical one-hot labels:\n# # loss_fn = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n# # otherwise for integer labels:\n# # loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T01:11:52.766145Z","iopub.execute_input":"2025-11-25T01:11:52.766333Z","iopub.status.idle":"2025-11-25T01:11:52.777642Z","shell.execute_reply.started":"2025-11-25T01:11:52.766313Z","shell.execute_reply":"2025-11-25T01:11:52.776972Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# PART 12 — Compile the Mode\n\nConcept\n:\n\t•\tSGD + Momentum → stable, proven optimizer for CNNs\n\t•\tLabel smoothing → prevents model from becoming overconfident\n\t•\tCompile → tie the model, loss, and optimizer together before trainin","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.datasets import cifar100\n\n(x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine')\n\ny_train = y_train.squeeze()\ny_test = y_test.squeeze()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T01:11:52.778424Z","iopub.execute_input":"2025-11-25T01:11:52.778687Z","iopub.status.idle":"2025-11-25T01:11:55.121425Z","shell.execute_reply.started":"2025-11-25T01:11:52.778670Z","shell.execute_reply":"2025-11-25T01:11:55.120631Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"steps_per_epoch = len(x_train) // BATCH_SIZE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T01:11:55.122298Z","iopub.execute_input":"2025-11-25T01:11:55.122692Z","iopub.status.idle":"2025-11-25T01:11:55.126448Z","shell.execute_reply.started":"2025-11-25T01:11:55.122660Z","shell.execute_reply":"2025-11-25T01:11:55.125810Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def se_block(x, se_ratio=8):\n    filters = x.shape[-1]\n    se = tf.keras.layers.GlobalAveragePooling2D()(x)\n    se = tf.keras.layers.Dense(filters // se_ratio, activation='relu')(se)\n    se = tf.keras.layers.Dense(filters, activation='sigmoid')(se)\n    se = tf.keras.layers.Reshape((1,1,filters))(se)\n    return tf.keras.layers.multiply([x, se])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T01:11:55.127152Z","iopub.execute_input":"2025-11-25T01:11:55.127373Z","iopub.status.idle":"2025-11-25T01:11:55.798560Z","shell.execute_reply.started":"2025-11-25T01:11:55.127356Z","shell.execute_reply":"2025-11-25T01:11:55.797687Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"steps_per_epoch = len(x_train) // BATCH_SIZE\nlr_schedule = WarmUpCosine(INITIAL_LR, EPOCHS, steps_per_epoch)\n\noptimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=MOMENTUM, nesterov=True)\nloss_fn = tf.keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING)\n\nmodel = build_custom_cnn()\nmodel.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T01:11:55.799355Z","iopub.execute_input":"2025-11-25T01:11:55.799583Z","iopub.status.idle":"2025-11-25T01:11:56.097819Z","shell.execute_reply.started":"2025-11-25T01:11:55.799566Z","shell.execute_reply":"2025-11-25T01:11:56.096669Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/3811859484.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategoricalCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLABEL_SMOOTHING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_custom_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/2344242944.py\u001b[0m in \u001b[0;36mbuild_custom_cnn\u001b[0;34m(input_shape, num_classes)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_custom_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_bn_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'layers' is not defined"],"ename":"NameError","evalue":"name 'layers' is not defined","output_type":"error"}],"execution_count":17},{"cell_type":"markdown","source":"# PART 13 — Callback\n\nConcept:\n\t•\tCheckpoint: saves your best weights\n\t•\tReduceLROnPlateau: lowers LR if loss stops improving\n\t•\tEarlyStopping: stops training early to prevent overfittin","metadata":{}},{"cell_type":"code","source":"callbacks = [\n    tf.keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_accuracy'),\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5),\n    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True)\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T01:11:56.098347Z","iopub.status.idle":"2025-11-25T01:11:56.098684Z","shell.execute_reply.started":"2025-11-25T01:11:56.098517Z","shell.execute_reply":"2025-11-25T01:11:56.098533Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Part 14: Train Model\n\nConcept:\n* Each epoch = full pass over train data\n* Validation used To Track Gen\n* Callback Help Tune Automatically While Training","metadata":{}},{"cell_type":"code","source":"AUTOTUNE = tf.data.AUTOTUNE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T01:11:56.099824Z","iopub.status.idle":"2025-11-25T01:11:56.100106Z","shell.execute_reply.started":"2025-11-25T01:11:56.099987Z","shell.execute_reply":"2025-11-25T01:11:56.100001Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use original integer labels y_train, y_test (shape (N,)) — do NOT one-hot encode\ntrain_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\ntrain_ds = train_ds.map(lambda x,y: (tf.cast(x, tf.float32)/255.0, y), num_parallel_calls=AUTOTUNE)\ntrain_ds = train_ds.map(preprocess_train, num_parallel_calls=AUTOTUNE)   # your augment fn must return (image, int_label)\ntrain_ds = train_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n\ntest_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\ntest_ds = test_ds.map(lambda x,y: (tf.cast(x, tf.float32)/255.0, y), num_parallel_calls=AUTOTUNE)\ntest_ds = test_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n\n# Compile with sparse loss (no label smoothing available on older TF)\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\nmodel.compile(optimizer=optimizer, loss=loss_fn, metrics=['sparse_categorical_accuracy'])\n# or metrics=['accuracy'] — Keras will infer correct accuracy for sparse targets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T01:11:56.101373Z","iopub.status.idle":"2025-11-25T01:11:56.101619Z","shell.execute_reply.started":"2025-11-25T01:11:56.101512Z","shell.execute_reply":"2025-11-25T01:11:56.101523Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1) Create optimizer with a float LR\noptimizer = tf.keras.optimizers.SGD(learning_rate=INITIAL_LR, momentum=MOMENTUM, nesterov=True)\n\n# 2) Define an epoch-based LR function (returns lr for given epoch)\ndef epoch_warmup_cosine(epoch):\n    # simple epoch-based schedule: linear warmup -> cosine decay across EPOCHS\n    warmup = WARMUP_EPOCHS\n    if epoch < warmup:\n        return float(INITIAL_LR * (epoch + 1) / warmup)  # +1 so epoch0 != 0\n    # cosine decay after warmup\n    progress = (epoch - warmup) / max(1, (EPOCHS - warmup))\n    cosine = 0.5 * (1.0 + math.cos(math.pi * min(1.0, progress)))\n    return float(INITIAL_LR * cosine)\n\n# 3) Use LearningRateScheduler (Keras will set optimizer.lr)\nlr_callback = tf.keras.callbacks.LearningRateScheduler(epoch_warmup_cosine, verbose=1)\n\n# 4) Use ReduceLROnPlateau as well (it will set lr when plateau occurs)\nreduce_on_plateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.5, patience=6, verbose=1, mode='min'\n)\n\ncallbacks = [\n    tf.keras.callbacks.ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1),\n    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True, mode='max', verbose=1),\n    lr_callback,\n    reduce_on_plateau\n]\n\nmodel.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T01:11:56.102597Z","iopub.status.idle":"2025-11-25T01:11:56.102804Z","shell.execute_reply.started":"2025-11-25T01:11:56.102703Z","shell.execute_reply":"2025-11-25T01:11:56.102712Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n    train_ds,\n    validation_data=test_ds,\n    epochs=EPOCHS,\n    callbacks=callbacks\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T01:11:56.103361Z","iopub.status.idle":"2025-11-25T01:11:56.104243Z","shell.execute_reply.started":"2025-11-25T01:11:56.104119Z","shell.execute_reply":"2025-11-25T01:11:56.104134Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**CHECK**","metadata":{}},{"cell_type":"code","source":"# 1) Print dataset batch shapes & label types\nxb, yb = next(iter(train_ds))\nprint(\"xb.shape, xb.min/max:\", xb.shape, float(tf.reduce_min(xb)), float(tf.reduce_max(xb)))\nprint(\"yb.shape, yb.dtype, unique labels (first 50):\", yb.shape, yb.dtype, tf.unique(tf.reshape(yb, [-1]))[0][:50])\n\n# 2) Show 6 images with labels (sanity)\nimport matplotlib.pyplot as plt\nfig, axes = plt.subplots(2,3, figsize=(9,6))\nfor i, ax in enumerate(axes.flatten()):\n    img = xb[i].numpy()\n    lbl = yb[i].numpy()\n    ax.imshow((img*255).astype('uint8') if img.max()<=1.0 else img.astype('uint8'))\n    ax.set_title(str(lbl))\n    ax.axis('off')\nplt.show()\n\n# 3) Model quick forward pass: check output shape & distribution\npreds = model.predict(xb[:64])\nprint(\"preds.shape:\", preds.shape)\nprint(\"preds min/max, mean:\", preds.min(), preds.max(), preds.mean())\nprint(\"argmax counts (first 64):\", np.bincount(np.argmax(preds, axis=1)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T01:11:56.105277Z","iopub.status.idle":"2025-11-25T01:11:56.105581Z","shell.execute_reply.started":"2025-11-25T01:11:56.105406Z","shell.execute_reply":"2025-11-25T01:11:56.105416Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# PART 15 — Evaluate and Visualize\n\nConcept:\n\n\t•\tLoad best model checkpoint\n\t•\tEvaluate on unseen test set\n\t•\tExpect accuracy to gradually climb toward 60–65% with good tunin","metadata":{}},{"cell_type":"code","source":"model.load_weights('best_model.h5')\ntest_loss, test_acc = model.evaluate(test_ds)\nprint(\"Test Accuracy:\", test_acc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T01:11:56.107022Z","iopub.status.idle":"2025-11-25T01:11:56.107344Z","shell.execute_reply.started":"2025-11-25T01:11:56.107181Z","shell.execute_reply":"2025-11-25T01:11:56.107195Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# FULL","metadata":{}},{"cell_type":"code","source":"import os\n\nbase = \"/kaggle/input/cifar100\"\nprint(\"Base exists:\", os.path.exists(base))\nprint(\"Top-level entries in /kaggle/input/cifar100:\\n\")\nfor name in sorted(os.listdir(base)):\n    path = os.path.join(base, name)\n    entry_type = \"DIR\" if os.path.isdir(path) else \"FILE\"\n    print(f\" - {name:40s}  ({entry_type})\")\n\n# Recursively list first-level subfolders and detect train/test-like directories\nprint(\"\\nSearching for candidate train/test directories (depth=2)...\\n\")\ncandidates = []\nfor root, dirs, files in os.walk(base):\n    # skip very deep traversal, only go 3 levels max\n    depth = root[len(base):].count(os.sep)\n    if depth > 3:\n        continue\n    # count image-like files\n    img_count = sum(1 for f in files if f.lower().endswith(('.png','.jpg','.jpeg','.bmp')))\n    if img_count > 0 or any(d.islower() for d in dirs):\n        candidates.append((root, len(dirs), img_count))\n# print some candidates (limited)\nfor root, ndirs, nimgs in sorted(candidates, key=lambda x: (-x[2], x[1]))[:40]:\n    print(f\"{root:80s}  | subdirs: {ndirs:3d}  | image_files_in_folder: {nimgs:4d}\")\n\n# Try auto-detect train/test\ntrain_dir = None\ntest_dir = None\nfor root, dirs, files in os.walk(base):\n    name = os.path.basename(root).lower()\n    if name == \"train\" and train_dir is None:\n        train_dir = root\n    if name == \"test\" and test_dir is None:\n        test_dir = root\n\n# Fallback: if not found, try to find directories containing many class-folders\nif train_dir is None or test_dir is None:\n    # look for folders that contain many subdirs (likely class folders)\n    cand = []\n    for root, dirs, files in os.walk(base):\n        if len(dirs) >= 20:   # heuristic: CIFAR100 has 100 classes, but directory split may vary\n            cand.append((root, len(dirs)))\n    cand = sorted(cand, key=lambda x: -x[1])\n    if cand:\n        print(\"\\nFolders that look like they contain class subfolders (candidates):\")\n        for r, nd in cand[:6]:\n            print(f\" - {r}  (subdirs: {nd})\")\n        # pick best candidate for train if explicit train/test not found\n        if train_dir is None:\n            train_dir = cand[0][0]\n\nprint(\"\\nRecommended values (if not None):\")\nprint(\"TRAIN_DIR =\", train_dir)\nprint(\"TEST_DIR  =\", test_dir)\n\nprint(\"\\nIf TRAIN_DIR/TEST_DIR are None or incorrect, paste the printed candidate paths here.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T01:38:44.396189Z","iopub.execute_input":"2025-11-25T01:38:44.397042Z","iopub.status.idle":"2025-11-25T01:38:44.411238Z","shell.execute_reply.started":"2025-11-25T01:38:44.397008Z","shell.execute_reply":"2025-11-25T01:38:44.410528Z"}},"outputs":[{"name":"stdout","text":"Base exists: True\nTop-level entries in /kaggle/input/cifar100:\n\n - test                                      (FILE)\n - train                                     (FILE)\n\nSearching for candidate train/test directories (depth=2)...\n\n\nRecommended values (if not None):\nTRAIN_DIR = None\nTEST_DIR  = None\n\nIf TRAIN_DIR/TEST_DIR are None or incorrect, paste the printed candidate paths here.\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# Kaggle-ready CIFAR-100 training cell\n# Assumes you uploaded folders: \n#   /kaggle/input/cifar100/train  (has class subfolders)\n#   /kaggle/input/cifar100/test   (has class subfolders)\n# If your test folder path differs, update TEST_DIR below.\n\nimport os\nimport math\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, regularizers\nimport matplotlib.pyplot as plt\n\n# --------- CONFIG ----------\nTRAIN_DIR = '/kaggle/input/cifar100/train'\nTEST_DIR  = '/kaggle/input/cifar100/test'   # change if different\nBATCH_SIZE = 128\nIMG_SIZE = (32, 32)\nNUM_CLASSES = 100\nEPOCHS = 60\nINITIAL_LR = 0.1\nMOMENTUM = 0.9\nWARMUP_EPOCHS = 5\nSEED = 42\nAUTOTUNE = tf.data.AUTOTUNE\n\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\n\n# --------- Load datasets from directories (labels are integer indices) ----------\nprint(\"Loading datasets from directories...\")\ntrain_ds = tf.keras.utils.image_dataset_from_directory(\n    TRAIN_DIR,\n    labels=\"inferred\",\n    label_mode=\"int\",\n    image_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    seed=SEED\n)\n\ntest_ds = tf.keras.utils.image_dataset_from_directory(\n    TEST_DIR,\n    labels=\"inferred\",\n    label_mode=\"int\",\n    image_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=False\n)\n\nclass_names = train_ds.class_names\nprint(\"Classes:\", len(class_names))\n\n# --------- Simple preprocessing and augmentation layers (Keras preprocessing) ----------\n# We'll only do small augmentations to avoid breaking ranges.\ndata_augment = tf.keras.Sequential([\n    layers.RandomFlip(\"horizontal\"),\n    layers.RandomTranslation(0.04, 0.04),   # small shift\n    # small random brightness/contrast can be added if desired:\n    # layers.RandomContrast(0.08),\n], name=\"data_augmentation\")\n\ndef preprocess_train(image, label):\n    image = tf.cast(image, tf.float32) / 255.0  # IMPORTANT: scale once to [0,1]\n    image = data_augment(image)\n    return image, label\n\ndef preprocess_eval(image, label):\n    image = tf.cast(image, tf.float32) / 255.0\n    return image, label\n\ntrain_ds = train_ds.map(preprocess_train, num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)\ntest_ds  = test_ds.map(preprocess_eval, num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)\n\n# Quick visual check (first batch)\nprint(\"\\nSanity check: sample images from train batch\")\nxb, yb = next(iter(train_ds))\nprint(\"batch shapes:\", xb.shape, yb.shape, \"min/max:\", xb.numpy().min(), xb.numpy().max())\nfig, axes = plt.subplots(2,6, figsize=(12,4))\naxes = axes.flatten()\nfor i, ax in enumerate(axes):\n    ax.imshow((xb[i].numpy()*255).astype('uint8'))\n    ax.set_title(int(yb[i].numpy()))\n    ax.axis('off')\nplt.show()\n\n# --------- Model: small ResNet-like (compact to run on Kaggle quickly) ----------\ndef conv_bn_relu(x, filters, kernel_size=3, stride=1):\n    x = layers.Conv2D(filters, kernel_size, strides=stride, padding='same', use_bias=False,\n                      kernel_regularizer=regularizers.l2(1e-4))(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n    return x\n\ndef residual_block(x, filters, stride=1):\n    shortcut = x\n    out = conv_bn_relu(x, filters, 3, stride)\n    out = layers.Conv2D(filters, 3, padding='same', use_bias=False,\n                        kernel_regularizer=regularizers.l2(1e-4))(out)\n    out = layers.BatchNormalization()(out)\n    if stride != 1 or x.shape[-1] != filters:\n        shortcut = layers.Conv2D(filters, 1, strides=stride, padding='same', use_bias=False,\n                                 kernel_regularizer=regularizers.l2(1e-4))(shortcut)\n        shortcut = layers.BatchNormalization()(shortcut)\n    out = layers.add([out, shortcut])\n    out = layers.ReLU()(out)\n    return out\n\ndef build_model(input_shape=(32,32,3), num_classes=100, dropout_rate=0.3):\n    inputs = layers.Input(shape=input_shape)\n    x = conv_bn_relu(inputs, 64, 3)\n    x = residual_block(x, 64)\n    x = residual_block(x, 128, stride=2)\n    x = residual_block(x, 128)\n    x = residual_block(x, 256, stride=2)\n    x = residual_block(x, 256)\n    # global pooling (avg)\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dropout(dropout_rate)(x)\n    x = layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(1e-4))(x)\n    x = layers.Dropout(dropout_rate)(x)\n    outputs = layers.Dense(num_classes, activation='softmax')(x)\n    model = models.Model(inputs, outputs, name='cifar_custom_resnet')\n    return model\n\nmodel = build_model(input_shape=(32,32,3), num_classes=len(class_names))\nmodel.summary()\n\n# --------- Epoch-based warmup + cosine LR scheduler (returns float per epoch) ----------\ndef epoch_warmup_cosine(epoch):\n    # epoch is 0-indexed\n    if epoch < WARMUP_EPOCHS:\n        # linear warmup\n        return float(INITIAL_LR * (epoch + 1) / WARMUP_EPOCHS)\n    else:\n        progress = (epoch - WARMUP_EPOCHS) / max(1, (EPOCHS - WARMUP_EPOCHS))\n        cosine = 0.5 * (1.0 + math.cos(math.pi * min(1.0, progress)))\n        return float(INITIAL_LR * cosine)\n\n# Make optimizer with float lr so callbacks that set lr are allowed\noptimizer = tf.keras.optimizers.SGD(learning_rate=INITIAL_LR, momentum=MOMENTUM, nesterov=True)\n\n# Use sparse loss (integers from image_dataset_from_directory)\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n\n# Compile\nmodel.compile(optimizer=optimizer, loss=loss_fn, metrics=['sparse_categorical_accuracy'])\n\n# --------- Callbacks ----------\ncheckpoint_path = '/kaggle/working/best_model.h5'\ncallbacks = [\n    tf.keras.callbacks.ModelCheckpoint(checkpoint_path, monitor='val_sparse_categorical_accuracy',\n                                       save_best_only=True, mode='max', verbose=1),\n    tf.keras.callbacks.EarlyStopping(monitor='val_sparse_categorical_accuracy', patience=20,\n                                     restore_best_weights=True, mode='max', verbose=1),\n    tf.keras.callbacks.LearningRateScheduler(epoch_warmup_cosine, verbose=1)\n]\n\n# --------- Train ----------\nhistory = model.fit(\n    train_ds,\n    validation_data=test_ds,\n    epochs=EPOCHS,\n    callbacks=callbacks,\n    verbose=2\n)\n\n# --------- Evaluate final model ----------\nprint(\"\\nLoading best checkpoint and evaluating on test set:\")\nmodel.load_weights(checkpoint_path)\nres = model.evaluate(test_ds, verbose=2)\nprint(\"Test loss, Test sparse accuracy:\", res)\n\n# Save the final model in Keras format (optional)\nmodel.save('/kaggle/working/custom_cifar100_model.keras')\nprint(\"Saved model to /kaggle/working/custom_cifar100_model.keras\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T01:33:31.132643Z","iopub.execute_input":"2025-11-25T01:33:31.133213Z","iopub.status.idle":"2025-11-25T01:33:31.164099Z","shell.execute_reply.started":"2025-11-25T01:33:31.133191Z","shell.execute_reply":"2025-11-25T01:33:31.163245Z"}},"outputs":[{"name":"stdout","text":"Loading datasets from directories...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/3272525036.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# --------- Load datasets from directories (labels are integer indices) ----------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading datasets from directories...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m train_ds = tf.keras.utils.image_dataset_from_directory(\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mTRAIN_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"inferred\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/image_dataset_utils.py\u001b[0m in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, pad_to_aspect_ratio, data_format, verbose)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     image_paths, labels, class_names = dataset_utils.index_directory(\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/dataset_utils.py\u001b[0m in \u001b[0;36mindex_directory\u001b[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links, verbose)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"inferred\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0msubdirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mlist_directory_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    766\u001b[0m   \"\"\"\n\u001b[1;32m    767\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m     raise errors.NotFoundError(\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0mnode_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0mop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotFoundError\u001b[0m: Could not find directory /kaggle/input/cifar100/train"],"ename":"NotFoundError","evalue":"Could not find directory /kaggle/input/cifar100/train","output_type":"error"}],"execution_count":23}]}